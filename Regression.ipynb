{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/MichaelQu/anaconda/lib/python3.5/site-packages/nltk/twitter/__init__.py:20: UserWarning: The twython library has not been installed. Some functionality from the twitter package will not be available.\n",
      "  warnings.warn(\"The twython library has not been installed. \"\n",
      "/Users/MichaelQu/anaconda/lib/python3.5/site-packages/statsmodels/compat/pandas.py:56: FutureWarning: The pandas.core.datetools module is deprecated and will be removed in a future version. Please use the pandas.tseries module instead.\n",
      "  from pandas.core import datetools\n"
     ]
    }
   ],
   "source": [
    "from newsapi import NewsApiClient\n",
    "from nytimesarticle import articleAPI\n",
    "import pandas_datareader.data as web\n",
    "import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import dateutil\n",
    "import time\n",
    "import quandl\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from pandas.plotting import autocorrelation_plot\n",
    "from pandas.tools.plotting import lag_plot\n",
    "from statsmodels.tsa.ar_model import AR\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import TimeSeriesSplit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Params\n",
    "# extra API keys, comment out as necessary\n",
    "api = articleAPI(\"b23351c6f9314694bfe4f4929a2b72c5\") \n",
    "#api = articleAPI(\"787bd4db8e704bbf9cebe8b7941827e0\") \n",
    "#api = articleAPI(\"f8b402f42ed14b249fd5accc95a050dd\") \n",
    "#api = articleAPI(\"c91a676aeaef40fd844409c8b0bef485\")\n",
    "#api = articleAPI(\"c43133d654134109868299ff505e7c55\")\n",
    "#api = articleAPI(\"eb427ebc2336423ead4d350cfa4e900b\")\n",
    "#api = articleAPI(\"b538de93f1a9459da22b150d7b53cb6f\")\n",
    "#api = articleAPI(\"88f587ed149d4478b4490168d61ed9dc\")\n",
    "\n",
    "quandl.ApiConfig.api_key = \"2S7d7eeL5VZrLup9pKg5\"\n",
    "end_date = (datetime.datetime.now() - datetime.timedelta(days=3)).isoformat()\n",
    "start_date = (datetime.datetime.now() - datetime.timedelta(days=365)).isoformat()\n",
    "left_sources = 'The New York Times'\n",
    "right_sources = 'Fox News'\n",
    "center_sources = 'Reuters AP The Wall Street Journal'\n",
    "all_sources = left_sources + ' ' + right_sources + ' ' + center_sources\n",
    "replace_list = ['Corp', 'Inc.', 'Inc', '.com', 'plc', ',', 'Co.']\n",
    "# domain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    # retrieve s&p 500 companies \n",
    "    df = pd.read_csv('constituents_csv.csv')\n",
    "    companies = df['Name']\n",
    "    company_symb = {}\n",
    "\n",
    "    # Iterate through companies\n",
    "    # for company in companies:\n",
    "    for i in range(30,31):\n",
    "\n",
    "        company = companies.loc[i]\n",
    "\n",
    "        # get company ticker\n",
    "        company_symb[company] = df[df['Name'] == company]['Symbol']\n",
    "        ticker = company_symb[company].values[0]\n",
    "        \n",
    "        # get rid of suffixes from company name\n",
    "        for word in replace_list:\n",
    "            company = company.replace(word, '')\n",
    "\n",
    "        # Read in data\n",
    "        df_master = read_in_stock_data(ticker, start_date, end_date)\n",
    "        \n",
    "        # Calculate 25 lags first\n",
    "        for i in range(25):\n",
    "            df_master['X_t-' + str(i+1)] = df_master['X_t'].shift(i+1)\n",
    "            \n",
    "        # Remove the ith data point because of shift\n",
    "        df_master = df_master.iloc[i+1:]\n",
    "        \n",
    "        ###### Add in sentiment values ######\n",
    "        \n",
    "        # Query news articles\n",
    "        trading_dates = df_master.index\n",
    "        dict_master = query_news_articles(company, start_date, end_date, trading_dates, sources=all_sources)\n",
    "        \n",
    "        # Add sentiment columns in dataframe\n",
    "        df_master['Pos_t-1'] = 0\n",
    "        df_master['Neu_t-1'] = 0\n",
    "        df_master['Neg_t-1'] = 0\n",
    "\n",
    "        # iterate through dates\n",
    "        for date in dict_master.keys():\n",
    "            # when you enter seniment into the dataframe, use the before date not after\n",
    "            average_sentiment_dict = calculate_sentiment(dict_master[date])\n",
    "\n",
    "            # Plug this into df\n",
    "            df_master.at[date,'Pos_t-1'] = average_sentiment_dict['pos']\n",
    "            df_master.at[date,'Neu_t-1'] = average_sentiment_dict['neu']\n",
    "            df_master.at[date,'Neg_t-1'] = average_sentiment_dict['neg']\n",
    "         \n",
    "        ###### Per Fold do cross validation ######\n",
    "        MSE_list_AR, coef_list_AR = cross_validate_AR(df_master)   \n",
    "        print('Mean Squared Error List:', MSE_list_AR)\n",
    "        print('Coefficient list:',coef_list_AR)\n",
    "        MSE_list_ADL, coef_list_ADL = cross_validate_ADL(df_master)\n",
    "        print('Mean Squared Error List:', MSE_list_ADL)\n",
    "        print('Coefficient list:',coef_list_ADL)\n",
    "        \n",
    "        #df_current.to_csv('./data/'+ticker+'.csv')\n",
    "        return df_master\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_read_in_csv(ticker):\n",
    "    df_master = pd.read_csv('/data'+ticker+'.csv')\n",
    "    \n",
    "    ###### Per Fold do cross validation ######\n",
    "    MSE_list_AR, coef_list_AR = cross_validate_AR(df_master)   \n",
    "    print('Mean Squared Error List:', MSE_list_AR)\n",
    "    print('Coefficient list:',coef_list_AR)\n",
    "    MSE_list_ADL, coef_list_ADL = cross_validate_ADL(df_master)\n",
    "    print('Mean Squared Error List:', MSE_list_ADL)\n",
    "    print('Coefficient list:',coef_list_ADL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "File b'/dataAMZN.csv' does not exist",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-a3b262ba83e9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmain_read_in_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'AMZN'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-18-d1c9fef59c00>\u001b[0m in \u001b[0;36mmain_read_in_csv\u001b[0;34m(ticker)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmain_read_in_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mticker\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mdf_master\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/data'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mticker\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;31m###### Per Fold do cross validation ######\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mMSE_list_AR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcoef_list_AR\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcross_validate_AR\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_master\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/MichaelQu/anaconda/lib/python3.5/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, skip_footer, doublequote, delim_whitespace, as_recarray, compact_ints, use_unsigned, low_memory, buffer_lines, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    653\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[1;32m    654\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 655\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    656\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    657\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/MichaelQu/anaconda/lib/python3.5/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    403\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    404\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 405\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    406\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    407\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/MichaelQu/anaconda/lib/python3.5/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    760\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    761\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 762\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    763\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    764\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/MichaelQu/anaconda/lib/python3.5/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m    964\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'c'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    965\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'c'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 966\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    967\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    968\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'python'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/MichaelQu/anaconda/lib/python3.5/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1580\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'allow_leading_cols'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex_col\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1581\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1582\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1583\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1584\u001b[0m         \u001b[0;31m# XXX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__ (pandas/_libs/parsers.c:4209)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source (pandas/_libs/parsers.c:8873)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: File b'/dataAMZN.csv' does not exist"
     ]
    }
   ],
   "source": [
    "main_read_in_csv('AMZN')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cross_validate_AR(df):\n",
    "    \"\"\" Runs the backtest with k fold cross validation for AR model\n",
    "    Params:\n",
    "        df (pd.Dataframe): Dataframe with X_t, lags, and sentiment score\n",
    "    Returns:\n",
    "        MSE_list (list): List of MSE per fold of cross-validation\n",
    "        coef_list (list): List of coef per fold of cross-validation\n",
    "    \"\"\"\n",
    "    tscv = TimeSeriesSplit(n_splits=8)\n",
    "    X = df.loc[:, df.columns !='X_t']\n",
    "    Y = df['X_t']\n",
    "    \n",
    "    MSE_list = []\n",
    "    coef_list = []\n",
    "        \n",
    "    for train_index, test_index in tscv.split(X):\n",
    "        X_train, X_test = X.iloc[train_index,:], X.iloc[test_index,:]\n",
    "        Y_train, Y_test = Y.iloc[train_index], Y[test_index]\n",
    "        \n",
    "        # find optimal lag period\n",
    "        lag = get_lag_period(Y_train)\n",
    "\n",
    "        # filter the lags and sentiment values\n",
    "        X_train = X_train.iloc[:,range(lag-1)]\n",
    "        X_test = X_test.iloc[:,range(lag-1)]\n",
    "\n",
    "        MSE, coef = fit_and_run_regression(X_train, Y_train, X_test, Y_test)\n",
    "        MSE_list.append(MSE)\n",
    "        coef_list.append(coef)\n",
    "        \n",
    "    return MSE_list, coef_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cross_validate_ADL(df):\n",
    "    \"\"\" Runs the backtest with k fold cross validation for ADL model\n",
    "    Params:\n",
    "        df (pd.Dataframe): Dataframe with X_t, lags, and sentiment score\n",
    "    Returns:\n",
    "        MSE_list (list): List of MSE per fold of cross-validation\n",
    "        coef_list (list): List of coef per fold of cross-validation\n",
    "    \"\"\"\n",
    "    tscv = TimeSeriesSplit(n_splits=8)\n",
    "    X = df.loc[:, df.columns !='X_t']\n",
    "    Y = df['X_t']\n",
    "    \n",
    "    MSE_list = []\n",
    "    coef_list = []\n",
    "        \n",
    "    for train_index, test_index in tscv.split(X):\n",
    "        X_train, X_test = X.iloc[train_index,:], X.iloc[test_index,:]\n",
    "        Y_train, Y_test = Y.iloc[train_index], Y[test_index]\n",
    "        \n",
    "        # find optimal lag period\n",
    "        lag = get_lag_period(Y_train)\n",
    "\n",
    "        # filter the lags and sentiment values\n",
    "        X_train = X_train.iloc[:,range(lag-1)]\n",
    "        X_test = X_test.iloc[:,range(lag-1)]\n",
    "        \n",
    "        # Add in the sentiment values\n",
    "        X_train['Pos_t-1'] = df.iloc[train_index,-3]\n",
    "        X_train['Neu_t-1'] = df.iloc[train_index,-2]\n",
    "        X_train['Neg_t-1'] = df.iloc[train_index,-1]\n",
    "        \n",
    "        X_test['Pos_t-1'] = df.iloc[test_index,-3]\n",
    "        X_test['Neu_t-1'] = df.iloc[test_index,-2]\n",
    "        X_test['Neg_t-1'] = df.iloc[test_index,-1]\n",
    "        \n",
    "        X_train = X_train.fillna(0)\n",
    "        X_test = X_test.fillna(0)\n",
    "\n",
    "        MSE, coef = fit_and_run_regression(X_train, Y_train, X_test, Y_test)\n",
    "        MSE_list.append(MSE)\n",
    "        coef_list.append(coef)\n",
    "        \n",
    "    return MSE_list, coef_list\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Methods Used for Data Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calculate_sentiment(sentence_arr):\n",
    "    \"\"\" Returns the average sentiment of the array\n",
    "    Params:\n",
    "        sentence_arr(Array): Array of setences that we have to calculate\n",
    "        the sentiment of.\n",
    "    Returns:\n",
    "        sentiment (dictionary): Takes the average of all sentences\n",
    "        format of score is {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}\n",
    "    \"\"\"\n",
    "    sia = SentimentIntensityAnalyzer()\n",
    "    df = df = pd.DataFrame(columns=['neg','neu','pos','compound'])\n",
    "    for sentence in sentence_arr:\n",
    "        sentiment = sia.polarity_scores(sentence)\n",
    "        df_sentiment = pd.DataFrame([sentiment], columns=sentiment.keys())\n",
    "        df = df.append(df_sentiment)\n",
    "    \n",
    "    avg_sentiment = dict(df.mean())    \n",
    "    return avg_sentiment "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def query_news_articles(company, start_date, end_date, trading_dates, sources):\n",
    "    \"\"\" Queries news article for a certain time frame and split it by dates\n",
    "        Note that\n",
    "    Params:\n",
    "        company (String): Name of company\n",
    "        start_date (String): Start date in format of \"2001-12-31\"\n",
    "        end_date (String): End date in format of \"2001-12-31\"\n",
    "         trading_dates (Array of Strings): Array of dates when the market was open\n",
    "                 dates in format of \"2001-12-31\"\n",
    "        sources (Array of Strings): Array of different news sources\n",
    "    Returns:\n",
    "        company_dic (dictionary): keys are date, values are array of headlines\n",
    "    \"\"\"\n",
    "    company_dict = {k: [] for k in trading_dates.date}\n",
    "    start_date = int(start_date.replace(\"-\",\"\").split('T')[0])\n",
    "    ending_date = (dateutil.parser.parse(end_date)-datetime.timedelta(days=1)).isoformat()\n",
    "    end_date = int(ending_date.replace(\"-\",\"\").split('T')[0])\n",
    "    newsdata = api.search(q=company, begin_date = start_date,\n",
    "                           end_date = end_date,\n",
    "                          fq='headline:('+company+ ') OR body:('+company+') AND source:(' + sources + ')',\n",
    "                          page = 0,\n",
    "                          facet_filter = True)\n",
    "                               \n",
    "\n",
    "    #print(newsdata) # newsdata is full HTTP response\n",
    "    number_of_hits = newsdata['response']['meta']['hits']\n",
    "    number_of_pages = (number_of_hits // 10) + 1\n",
    "    \n",
    "    time.sleep(1)\n",
    "    # page through results and add headlines to companY_dict\n",
    "    for i in range(0, min(number_of_pages,50)):\n",
    "        print('page', i)\n",
    "        newsdata = api.search(q=company, begin_date = start_date,\n",
    "                           end_date = end_date,\n",
    "                          fq='headline:('+company+ ') OR body:('+company+') AND source:(' + sources + ')',\n",
    "                          page = i,\n",
    "                          facet_filter = True)\n",
    "        articles = newsdata['response']['docs']\n",
    "        for article in articles:\n",
    "            relevance = article['score']\n",
    "            if relevance >= 0.005: \n",
    "                headline = article['headline']['main']\n",
    "                blurb = article['snippet']\n",
    "                # print(article['pub_date'], '\\t', article['headline']['main'])\n",
    "            \n",
    "                # description = article['description']\n",
    "                # format of date is 2018-04-13T00:46:59Z (UTC format)\n",
    "                publish_date = article['pub_date'] \n",
    "                # adjust date for trading day\n",
    "                publish_date, publish_time = publish_date.split('T')\n",
    "                date_arr = publish_date.split('-')\n",
    "                publish_datetime = datetime.date(int(date_arr[0]), int(date_arr[1]), int(date_arr[2]))\n",
    "                time_arr = publish_time[:-1].split(':')\n",
    "                # stock market closes at 4:00 PM EST; if article published after \n",
    "                # 16:00:00+4:00:00 = 20:00:00 UTC headline affects next trading day;\n",
    "                # otherwise affects current trading day\n",
    "                trading_datetime = publish_datetime\n",
    "                if int(time_arr[0]) >= 20:\n",
    "                    trading_datetime += datetime.timedelta(days=1)\n",
    "                \n",
    "                # if given trading_date invalid (ie if article published on Friday \n",
    "                # after market close, Saturday, or Sunday before 4 pm est) push trading_date\n",
    "                # to the following Monday (ie first valid trading_date)\n",
    "                while trading_datetime not in trading_dates:\n",
    "                    trading_datetime += datetime.timedelta(1)\n",
    "                company_dict[trading_datetime].append(headline)\n",
    "                # company_dict[trading_datetime].append(blurb) include 'snippet' in sentiment analysis\n",
    "        time.sleep(1)\n",
    "        \n",
    "    return company_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_lag_period(X_train):\n",
    "    \"\"\" Finds the optimal lab period given a ticker and the start date and end date\n",
    "    Params:\n",
    "        X_train (df): Training data used to calculate lag back\n",
    "    Returns:\n",
    "        lag (int): Number of lag periods\n",
    "    \"\"\"\n",
    "    model = AR(X_train)\n",
    "    model_fit = model.fit()\n",
    "    lag = model_fit.k_ar\n",
    "    \n",
    "    return lag\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_in_stock_data(ticker, start_date, end_date):\n",
    "    \"\"\" Initializes a data frame for a certain ticker\n",
    "    Params:\n",
    "        ticker (String): Stock ticker to be analyzed\n",
    "        start_date (String): Start date in format of \"2001-12-31\"\n",
    "        end_date (String): End date in format of \"2001-12-31\"\n",
    "        lag (int): Number of lag periods\n",
    "    Returns:\n",
    "        dataframe (pd.Dataframe): Dataframe with index 'Date' and column 'X_t'\n",
    "    \"\"\"\n",
    "    # Get the data in a dataframe\n",
    "    dataframe = web.DataReader(ticker, 'morningstar', start_date, end_date)['Close']\n",
    "    dataframe = pd.Series.to_frame(dataframe)\n",
    "    dataframe.reset_index(level=0, drop=True, inplace=True)\n",
    "    dataframe.columns = ['X_t']\n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fit_and_run_regression(X_train, Y_train, X_test, Y_test, doprint=False):\n",
    "    \"\"\" Fits the model with train and test data\n",
    "    Params:\n",
    "        X_train (pd.Dataframe): X training data\n",
    "        Y_train (pd.Dataframe): Y training data\n",
    "    Returns:\n",
    "        MSE (float): Mean Squared Error \n",
    "    \"\"\"\n",
    "    LinReg = LinearRegression(normalize=True)\n",
    "    LinReg.fit(X_train,Y_train)\n",
    "    Y_pred = LinReg.predict(X_test)\n",
    "    MSE = mean_squared_error(Y_test, Y_pred)\n",
    "    coefficients = LinReg.coef_\n",
    "    if doprint:\n",
    "        print(\"R^2 Value: %.2f\" %LinReg.score(X_test, Y_test))\n",
    "        print(\"Mean squared error: %.2f \"% MSE)\n",
    "        print(\"Coefficients for the Regression are: \" ,coefficients)\n",
    "    \n",
    "    return MSE, coefficients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try AR Regression without the package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "get_lag_period() takes 1 positional argument but 3 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-a363c66c5d19>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Try normal linear regression using lag period lags\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mlag\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_lag_period\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'AMZN'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_date\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_date\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Number of lags:'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlag\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mdf_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minitialize_dataframe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'AMZN'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_date\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_date\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlag\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mdf_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: get_lag_period() takes 1 positional argument but 3 were given"
     ]
    }
   ],
   "source": [
    "# Try normal linear regression using lag period lags\n",
    "lag = get_lag_period('AMZN', start_date, end_date)\n",
    "print('Number of lags:',lag)\n",
    "df_test = initialize_dataframe('AMZN', start_date, end_date, lag)\n",
    "df_test.head()\n",
    "\n",
    "Y = df_test['X_t']\n",
    "X = df_test.loc[:, df_test.columns !='X_t']\n",
    "X_train, X_test = X[:int(len(X)*(0.8))], X[int(len(X)*(0.8)):]\n",
    "Y_train, Y_test = Y[:int(len(Y)*(0.8))], Y[int(len(Y)*(0.8)):]\n",
    "\n",
    "LinReg = LinearRegression(normalize=True)\n",
    "LinReg.fit(X_train,Y_train)\n",
    "Y_pred = LinReg.predict(X_test)\n",
    "print(\"R^2 Value: %.2f\" %LinReg.score(X_test, Y_test))\n",
    "print(\"Mean squared error: %.2f\"\n",
    "      % mean_squared_error(Y_test, Y_pred))\n",
    "print(\"Coefficients for the Regression are: \" ,LinReg.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Y_test' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-797288d6c639>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mY_plot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mY_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdeep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mY_plot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mY_plot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_frame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mY_plot\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Predicted'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSeries\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mY_plot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mY_plot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'Actual'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Predicted'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY_plot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Y_test' is not defined"
     ]
    }
   ],
   "source": [
    "Y_plot = Y_test.copy(deep=True)\n",
    "Y_plot = Y_plot.to_frame()\n",
    "Y_plot['Predicted'] = pd.Series(Y_pred, index=Y_plot.index)\n",
    "Y_plot.columns = ['Actual', 'Predicted']\n",
    "print(Y_plot.head())\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.title('Predicted Vs. Actual Plot AR model')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Price')\n",
    "plt.plot(Y_plot.index, Y_plot['Actual'], label='Actual')\n",
    "plt.plot(Y_plot.index, Y_plot['Predicted'],label='Predicted')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try Regression with sentiment analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_sentiment = main()\n",
    "df_sentiment.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Do the regression sentiment\n",
    "\n",
    "# Count number of nulls and replace them with 0's\n",
    "df_sentiment.isnull().sum()\n",
    "df_sentiment = df_sentiment.fillna(0)\n",
    "df_sentiment.head()\n",
    "\n",
    "Y = df_sentiment['X_t']\n",
    "X = df_sentiment.loc[:, df_sentiment.columns !='X_t']\n",
    "X_train, X_test = X[:int(len(X)*(0.8))], X[int(len(X)*(0.8)):]\n",
    "Y_train, Y_test = Y[:int(len(Y)*(0.8))], Y[int(len(Y)*(0.8)):]\n",
    "\n",
    "LinReg_sentiment = LinearRegression(normalize=True)\n",
    "LinReg_sentiment.fit(X_train,Y_train)\n",
    "Y_pred = LinReg_sentiment.predict(X_test)\n",
    "print(\"R^2 Value: %.2f\" %LinReg_sentiment.score(X_test, Y_test))\n",
    "print(\"Mean squared error: %.2f\"\n",
    "      % mean_squared_error(Y_test, Y_pred))\n",
    "print(\"Coefficients for the Regression are: \" , LinReg_sentiment.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Plot\n",
    "Y_plot = Y_test.copy(deep=True)\n",
    "Y_plot = Y_plot.to_frame()\n",
    "Y_plot['Predicted'] = pd.Series(Y_pred, index=Y_plot.index)\n",
    "Y_plot.columns = ['Actual', 'Predicted']\n",
    "print(Y_plot.head())\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.title('Predicted Vs. Actual Plot ADL Model')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Price')\n",
    "plt.plot(Y_plot.index, Y_plot['Actual'], label='Actual')\n",
    "plt.plot(Y_plot.index, Y_plot['Predicted'],label='Predicted')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Run to get amazon's df\n",
    "df_current = main()\n",
    "df_current.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Find index of earlier data\n",
    "df_current.loc[datetime.datetime(2018,2,7)]\n",
    "# df_current.loc[df_current['Date'] == datetime.datetime(2018,2,7)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Use this to splice the dataframe to 2018-02-07 and after\n",
    "df_changed = df_current[456:].fillna(0)\n",
    "df_changed.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Plot autocorrelation values\n",
    "plt.figure()\n",
    "autocorrelation_plot(df_changed['X_t'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Split into train and test\n",
    "X = df_changed['X_t']\n",
    "train, test = X[:int(len(X)*(0.8))], X[int(len(X)*(0.8)):]\n",
    "# Fit the model\n",
    "model = AR(train)\n",
    "model_fit = model.fit()\n",
    "print('Lag: %s' % model_fit.k_ar)\n",
    "print('Coefficients: %s' % model_fit.params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# make predictions\n",
    "predictions = model_fit.predict(start=len(train), end=len(train)+len(test)-1, dynamic=False)\n",
    "for i in range(len(predictions)):\n",
    "    print('predicted=%f, expected=%f' % (predictions[i], test[i]))\n",
    "error = mean_squared_error(test, predictions)\n",
    "print('Test MSE: %.3f' % error)\n",
    "R2 = r2_score(test, predictions)\n",
    "print('Test R^2 Score: %.3f ' % R2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Plot the preditions\n",
    "plt.figure(figsize=(10,7))\n",
    "plt.plot(test, label='Actual Values')\n",
    "plt.title('Predicted vs. Actual Values')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Price')\n",
    "plt.plot(predictions, label='Predicted Values')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [anaconda]",
   "language": "python",
   "name": "Python [anaconda]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
