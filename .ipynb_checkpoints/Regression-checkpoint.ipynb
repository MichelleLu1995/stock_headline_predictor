{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/MichaelQu/anaconda/lib/python3.5/site-packages/nltk/twitter/__init__.py:20: UserWarning: The twython library has not been installed. Some functionality from the twitter package will not be available.\n",
      "  warnings.warn(\"The twython library has not been installed. \"\n",
      "/Users/MichaelQu/anaconda/lib/python3.5/site-packages/statsmodels/compat/pandas.py:56: FutureWarning: The pandas.core.datetools module is deprecated and will be removed in a future version. Please use the pandas.tseries module instead.\n",
      "  from pandas.core import datetools\n"
     ]
    }
   ],
   "source": [
    "from newsapi import NewsApiClient\n",
    "from nytimesarticle import articleAPI\n",
    "import pandas_datareader.data as web\n",
    "import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import dateutil\n",
    "import time\n",
    "import quandl\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from pandas.plotting import autocorrelation_plot\n",
    "from pandas.tools.plotting import lag_plot\n",
    "from statsmodels.tsa.ar_model import AR\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import TimeSeriesSplit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Params\n",
    "# extra API keys, comment out as necessary\n",
    "api = articleAPI(\"b23351c6f9314694bfe4f4929a2b72c5\") \n",
    "#api = articleAPI(\"787bd4db8e704bbf9cebe8b7941827e0\") \n",
    "#api = articleAPI(\"f8b402f42ed14b249fd5accc95a050dd\") \n",
    "#api = articleAPI(\"c91a676aeaef40fd844409c8b0bef485\")\n",
    "#api = articleAPI(\"c43133d654134109868299ff505e7c55\")\n",
    "#api = articleAPI(\"eb427ebc2336423ead4d350cfa4e900b\")\n",
    "#api = articleAPI(\"b538de93f1a9459da22b150d7b53cb6f\")\n",
    "#api = articleAPI(\"88f587ed149d4478b4490168d61ed9dc\")\n",
    "\n",
    "quandl.ApiConfig.api_key = \"2S7d7eeL5VZrLup9pKg5\"\n",
    "end_date = (datetime.datetime.now() - datetime.timedelta(days=3)).isoformat()\n",
    "start_date = (datetime.datetime.now() - datetime.timedelta(days=365)).isoformat()\n",
    "left_sources = 'The New York Times'\n",
    "right_sources = 'Fox News'\n",
    "center_sources = 'Reuters AP The Wall Street Journal'\n",
    "all_sources = left_sources + ' ' + right_sources + ' ' + center_sources\n",
    "replace_list = ['Corp', 'Inc.', 'Inc', '.com', 'plc', ',', 'Co.']\n",
    "# domain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    # retrieve s&p 500 companies \n",
    "    df = pd.read_csv('constituents_csv.csv')\n",
    "    companies = df['Name']\n",
    "    company_symb = {}\n",
    "\n",
    "    # Iterate through companies\n",
    "    # for company in companies:\n",
    "    for i in range(30,31):\n",
    "\n",
    "        company = companies.loc[i]\n",
    "\n",
    "        # get company ticker\n",
    "        company_symb[company] = df[df['Name'] == company]['Symbol']\n",
    "        ticker = company_symb[company].values[0]\n",
    "        \n",
    "        # get rid of suffixes from company name\n",
    "        for word in replace_list:\n",
    "            company = company.replace(word, '')\n",
    "\n",
    "        # Read in data\n",
    "        df_master = read_in_stock_data(ticker, start_date, end_date)\n",
    "        \n",
    "        # Calculate 25 lags first\n",
    "        for i in range(25):\n",
    "            df_master['X_t-' + str(i+1)] = df_master['X_t'].shift(i+1)\n",
    "            \n",
    "        # Remove the ith data point because of shift\n",
    "        df_master = df_master.iloc[i+1:]\n",
    "        \n",
    "        ###### Add in sentiment values ######\n",
    "        \n",
    "        # Query news articles\n",
    "        trading_dates = df_master.index\n",
    "        dict_master = query_news_articles(company, start_date, end_date, trading_dates, sources=all_sources)\n",
    "        \n",
    "        # Add sentiment columns in dataframe\n",
    "        df_master['Pos_t-1'] = 0\n",
    "        df_master['Neu_t-1'] = 0\n",
    "        df_master['Neg_t-1'] = 0\n",
    "\n",
    "        # iterate through dates\n",
    "        for date in dict_master.keys():\n",
    "            # when you enter seniment into the dataframe, use the before date not after\n",
    "            average_sentiment_dict = calculate_sentiment(dict_master[date])\n",
    "\n",
    "            # Plug this into df\n",
    "            df_master.at[date,'Pos_t-1'] = average_sentiment_dict['pos']\n",
    "            df_master.at[date,'Neu_t-1'] = average_sentiment_dict['neu']\n",
    "            df_master.at[date,'Neg_t-1'] = average_sentiment_dict['neg']\n",
    "         \n",
    "        ###### Per Fold do cross validation ######\n",
    "        MSE_list_AR, coef_list_AR = cross_validate_AR(df_master)   \n",
    "        print('Mean Squared Error List:', MSE_list_AR)\n",
    "        print('Coefficient list:',coef_list_AR)\n",
    "        MSE_list_ADL, coef_list_ADL = cross_validate_ADL(df_master)\n",
    "        print('Mean Squared Error List:', MSE_list_ADL)\n",
    "        print('Coefficient list:',coef_list_ADL)\n",
    "        \n",
    "        #df_current.to_csv('./data/'+ticker+'.csv')\n",
    "        return df_master\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page 0\n",
      "2018-04-13T17:09:19+0000\n",
      "2017-09-18T12:59:01+0000\n",
      "2018-05-03T18:54:19+0000\n",
      "2018-05-03T00:46:35+0000\n",
      "2018-05-03T00:42:54+0000\n",
      "2018-03-30T15:45:21+0000\n",
      "2018-05-01T23:31:12+0000\n",
      "2017-08-24T21:30:08+0000\n",
      "2018-04-27T20:46:43+0000\n",
      "2018-05-05T20:46:34+0000\n",
      "page 1\n",
      "2018-05-01T14:34:39+0000\n",
      "2018-05-03T19:55:32+0000\n",
      "2018-05-03T06:37:35+0000\n",
      "2018-05-01T07:10:43+0000\n",
      "2018-04-30T23:16:32+0000\n",
      "2018-05-02T08:22:32+0000\n",
      "2018-05-02T20:25:47+0000\n",
      "2018-05-01T03:40:38+0000\n",
      "2018-04-04T09:11:09+0000\n",
      "2018-04-30T23:19:32+0000\n",
      "page 2\n",
      "2018-04-27T11:04:37+0000\n",
      "2018-04-27T13:43:37+0000\n",
      "2018-05-01T21:02:08+0000\n",
      "2018-04-27T12:46:35+0000\n",
      "2018-04-27T04:01:41+0000\n",
      "2018-04-27T12:07:15+0000\n",
      "2018-04-27T14:52:38+0000\n",
      "2018-03-29T10:40:59+0000\n",
      "2018-04-27T04:34:37+0000\n",
      "2018-04-27T06:22:41+0000\n",
      "page 3\n",
      "2018-05-05T18:22:32+0000\n",
      "2018-03-29T12:34:22+0000\n",
      "2017-11-10T19:30:01+0000\n",
      "2018-04-01T21:52:10+0000\n",
      "2018-03-07T14:00:27+0000\n",
      "2018-04-03T00:02:02+0000\n",
      "2018-04-25T18:33:07+0000\n",
      "2018-04-24T11:00:05+0000\n",
      "2018-01-31T19:12:16+0000\n",
      "2018-02-27T21:46:58+0000\n",
      "page 4\n",
      "2018-03-01T10:01:11+0000\n",
      "2018-03-18T17:52:32+0000\n",
      "2018-01-11T10:01:42+0000\n",
      "2018-04-18T23:16:31+0000\n",
      "2018-01-25T18:11:56+0000\n",
      "2018-02-25T20:56:09+0000\n",
      "2018-04-26T23:38:53+0000\n",
      "2018-01-18T23:55:29+0000\n",
      "2018-04-10T21:51:20+0000\n",
      "2018-03-08T21:44:36+0000\n",
      "page 5\n",
      "2018-04-04T23:27:59+0000\n",
      "2018-02-15T23:39:27+0000\n",
      "2018-04-04T00:46:05+0000\n",
      "2018-03-29T22:01:32+0000\n",
      "2018-02-09T19:48:18+0000\n",
      "2018-02-09T19:00:01+0000\n",
      "2018-03-01T22:48:51+0000\n",
      "2018-02-04T21:11:26+0000\n",
      "2018-02-01T17:48:38+0000\n",
      "2018-02-03T00:46:52+0000\n",
      "page 6\n",
      "2018-01-23T09:00:25+0000\n",
      "2018-02-02T00:46:23+0000\n",
      "2018-02-14T05:01:12+0000\n",
      "2017-12-29T15:52:31+0000\n",
      "2018-01-18T14:05:33+0000\n",
      "2018-01-18T18:39:57+0000\n",
      "2018-03-15T23:20:22+0000\n",
      "2018-01-18T19:13:23+0000\n",
      "2018-03-25T21:30:06+0000\n",
      "2018-01-19T10:00:24+0000\n",
      "page 7\n",
      "2018-01-18T17:14:22+0000\n",
      "2018-02-02T14:00:01+0000\n",
      "2018-02-01T21:44:29+0000\n",
      "2018-02-03T01:53:18+0000\n",
      "2018-02-01T12:29:50+0000\n",
      "2018-01-30T12:22:10+0000\n",
      "2018-01-21T15:00:40+0000\n",
      "2017-09-18T09:00:01+0000\n",
      "2018-01-22T10:20:57+0000\n",
      "2018-01-18T23:10:01+0000\n",
      "page 8\n",
      "2018-01-19T10:40:46+0000\n",
      "2018-01-12T20:08:29+0000\n",
      "2017-12-26T10:00:41+0000\n",
      "2017-11-13T19:54:18+0000\n",
      "2017-12-08T17:30:35+0000\n",
      "2018-01-28T19:30:49+0000\n",
      "2018-01-02T20:51:45+0000\n",
      "2018-01-31T00:06:19+0000\n",
      "2018-01-31T23:22:49+0000\n",
      "2018-01-26T22:59:51+0000\n",
      "page 9\n",
      "2017-12-05T10:23:35+0000\n",
      "2017-10-19T13:40:49+0000\n",
      "2017-12-06T10:00:43+0000\n",
      "2017-11-30T00:06:28+0000\n",
      "2017-11-28T21:03:02+0000\n",
      "2017-11-26T19:27:48+0000\n",
      "2017-09-26T15:46:35+0000\n",
      "2017-10-26T03:06:20+0000\n",
      "2017-11-16T16:56:23+0000\n",
      "2018-01-30T12:46:41+0000\n",
      "page 10\n",
      "2017-10-19T18:30:18+0000\n",
      "2017-10-20T08:55:24+0000\n",
      "2018-01-18T23:56:18+0000\n",
      "2017-12-01T16:35:21+0000\n",
      "2017-10-27T19:14:51+0000\n",
      "2017-09-30T17:31:50+0000\n",
      "2017-08-30T23:54:44+0000\n",
      "2017-11-16T10:00:08+0000\n",
      "2017-09-08T12:14:56+0000\n",
      "2017-10-22T18:48:26+0000\n",
      "page 11\n",
      "2017-10-20T23:41:34+0000\n",
      "2017-10-17T22:13:51+0000\n",
      "2017-10-13T16:54:29+0000\n",
      "2017-12-05T04:02:26+0000\n",
      "2017-10-15T17:59:31+0000\n",
      "2017-10-16T16:49:37+0000\n",
      "2017-09-10T21:33:31+0000\n",
      "2017-10-13T00:12:21+0000\n",
      "2017-11-29T06:00:30+0000\n",
      "2017-09-09T05:44:00+0000\n",
      "page 12\n",
      "2017-10-27T21:14:12+0000\n",
      "2017-10-26T21:45:11+0000\n",
      "2017-10-30T20:46:17+0000\n",
      "2017-11-15T23:49:57+0000\n",
      "2017-10-18T22:35:40+0000\n",
      "2017-08-28T16:01:55+0000\n",
      "2017-09-22T14:25:31+0000\n",
      "2017-11-17T03:18:17+0000\n",
      "2017-10-02T21:24:21+0000\n",
      "2017-10-12T07:21:13+0000\n",
      "page 13\n",
      "2017-09-10T22:51:45+0000\n",
      "2017-09-19T10:31:18+0000\n",
      "2017-09-18T21:24:33+0000\n",
      "2017-09-10T21:33:19+0000\n",
      "2017-09-15T15:14:31+0000\n",
      "2017-08-23T04:01:04+0000\n",
      "2017-07-21T12:26:12+0000\n",
      "2017-09-15T18:48:16+0000\n",
      "2017-10-19T11:00:02+0000\n",
      "2017-10-03T17:30:46+0000\n",
      "page 14\n",
      "2017-09-15T22:40:05+0000\n",
      "2018-04-19T14:30:05+0000\n",
      "2017-09-27T23:06:37+0000\n",
      "2017-10-27T23:57:31+0000\n",
      "2018-04-20T08:00:01+0000\n",
      "2017-10-01T11:00:03+0000\n",
      "2017-08-03T20:42:13+0000\n",
      "2017-10-25T22:40:59+0000\n",
      "2017-10-09T16:02:55+0000\n",
      "2017-10-28T10:00:02+0000\n",
      "page 15\n",
      "2017-10-27T01:21:07+0000\n",
      "2017-08-30T22:10:39+0000\n",
      "2017-06-11T23:31:11+0000\n",
      "2017-10-24T14:11:00+0000\n",
      "2017-10-04T18:18:48+0000\n",
      "2017-09-20T12:54:15+0000\n",
      "2017-07-06T16:33:39+0000\n",
      "2017-08-12T18:30:11+0000\n",
      "2017-10-23T21:05:34+0000\n",
      "2017-10-13T09:00:22+0000\n",
      "page 16\n",
      "2017-09-09T09:00:03+0000\n",
      "2017-08-23T23:57:58+0000\n",
      "2017-09-08T15:59:28+0000\n",
      "2017-08-16T20:55:01+0000\n",
      "2017-09-07T20:00:38+0000\n",
      "2017-10-04T09:48:46+0000\n",
      "2017-08-30T07:00:21+0000\n",
      "2017-06-20T15:57:50+0000\n",
      "2017-08-31T18:33:30+0000\n",
      "2017-06-20T15:24:13+0000\n",
      "page 17\n",
      "2017-09-25T09:00:26+0000\n",
      "2017-06-16T23:36:48+0000\n",
      "2017-09-01T15:27:25+0000\n",
      "2017-07-26T08:00:52+0000\n",
      "2017-07-28T01:16:25+0000\n",
      "2017-07-18T09:00:05+0000\n",
      "2017-05-11T09:00:39+0000\n",
      "2017-06-25T21:17:00+0000\n",
      "2017-09-07T11:21:53+0000\n",
      "2017-08-03T09:35:03+0000\n",
      "page 18\n",
      "2017-07-31T20:26:24+0000\n",
      "2017-08-02T18:10:03+0000\n",
      "2017-08-28T22:26:52+0000\n",
      "2017-07-28T20:06:11+0000\n",
      "2018-02-14T10:00:50+0000\n",
      "2017-07-20T09:00:38+0000\n",
      "2017-08-18T09:00:32+0000\n",
      "2017-06-20T00:00:34+0000\n",
      "2017-06-16T11:53:56+0000\n",
      "2017-07-11T16:34:36+0000\n",
      "page 19\n",
      "2017-06-16T13:15:08+0000\n",
      "2017-06-19T13:33:41+0000\n",
      "2017-06-07T01:00:16+0000\n",
      "2017-08-01T20:19:38+0000\n",
      "2017-08-21T01:00:03+0000\n",
      "2017-06-29T17:25:18+0000\n",
      "2017-06-29T01:58:02+0000\n",
      "2018-01-19T19:20:40+0000\n",
      "2017-12-26T09:30:23+0000\n",
      "2017-06-20T15:03:58+0000\n",
      "page 20\n",
      "2017-06-16T19:19:55+0000\n",
      "2017-06-17T13:00:04+0000\n",
      "2018-01-03T18:32:27+0000\n",
      "2017-05-24T07:30:26+0000\n",
      "2017-05-18T16:52:14+0000\n",
      "2018-02-08T17:11:39+0000\n",
      "2017-06-01T17:26:49+0000\n",
      "2017-06-21T07:21:06+0000\n",
      "2017-06-17T16:18:22+0000\n",
      "2017-06-16T14:33:38+0000\n",
      "page 21\n",
      "2017-06-19T18:15:23+0000\n",
      "2017-06-16T22:06:46+0000\n",
      "2017-05-30T20:38:24+0000\n",
      "2017-05-16T00:42:30+0000\n",
      "2017-05-10T04:01:18+0000\n",
      "2018-01-17T15:20:00+0000\n",
      "2017-08-25T19:04:35+0000\n",
      "2017-09-23T20:37:59+0000\n",
      "2018-05-04T11:10:36+0000\n",
      "2017-05-19T16:29:09+0000\n",
      "page 22\n",
      "2018-04-22T20:24:34+0000\n",
      "2018-04-29T21:03:21+0000\n",
      "page 23\n",
      "page 24\n",
      "page 25\n",
      "page 26\n",
      "page 27\n",
      "page 28\n",
      "page 29\n",
      "page 30\n",
      "page 31\n",
      "page 32\n",
      "page 33\n",
      "page 34\n",
      "page 35\n",
      "page 36\n",
      "page 37\n",
      "page 38\n",
      "page 39\n",
      "page 40\n",
      "page 41\n",
      "page 42\n",
      "page 43\n",
      "page 44\n",
      "page 45\n",
      "page 46\n",
      "page 47\n",
      "page 48\n",
      "page 49\n",
      "page 50\n",
      "page 51\n",
      "page 52\n",
      "page 53\n",
      "page 54\n",
      "page 55\n",
      "page 56\n",
      "page 57\n",
      "page 58\n",
      "page 59\n",
      "page 60\n",
      "page 61\n",
      "page 62\n",
      "page 63\n",
      "page 64\n",
      "page 65\n",
      "page 66\n",
      "page 67\n",
      "page 68\n",
      "page 69\n",
      "page 70\n",
      "page 71\n",
      "page 72\n",
      "page 73\n",
      "page 74\n",
      "page 75\n",
      "page 76\n",
      "page 77\n",
      "page 78\n",
      "page 79\n",
      "page 80\n",
      "page 81\n",
      "page 82\n",
      "page 83\n",
      "page 84\n",
      "page 85\n",
      "page 86\n",
      "page 87\n",
      "page 88\n",
      "page 89\n",
      "page 90\n",
      "page 91\n",
      "page 92\n",
      "page 93\n",
      "page 94\n",
      "page 95\n",
      "page 96\n",
      "page 97\n",
      "page 98\n",
      "page 99\n",
      "page 100\n",
      "page 101\n",
      "page 102\n",
      "page 103\n",
      "page 104\n",
      "page 105\n",
      "page 106\n",
      "page 107\n",
      "page 108\n",
      "page 109\n",
      "page 110\n",
      "page 111\n",
      "page 112\n",
      "page 113\n",
      "page 114\n",
      "page 115\n",
      "page 116\n",
      "page 117\n",
      "page 118\n",
      "page 119\n",
      "page 120\n",
      "page 121\n",
      "page 122\n",
      "page 123\n",
      "page 124\n",
      "page 125\n",
      "page 126\n",
      "page 127\n",
      "page 128\n",
      "page 129\n",
      "page 130\n",
      "page 131\n",
      "page 132\n",
      "page 133\n",
      "page 134\n",
      "page 135\n",
      "page 136\n",
      "page 137\n",
      "page 138\n",
      "page 139\n",
      "page 140\n",
      "page 141\n",
      "page 142\n",
      "page 143\n",
      "page 144\n",
      "page 145\n",
      "page 146\n",
      "page 147\n",
      "page 148\n",
      "page 149\n",
      "page 150\n",
      "page 151\n",
      "page 152\n",
      "page 153\n",
      "page 154\n",
      "page 155\n",
      "page 156\n",
      "page 157\n",
      "page 158\n",
      "page 159\n",
      "page 160\n",
      "page 161\n",
      "page 162\n",
      "page 163\n",
      "page 164\n",
      "page 165\n",
      "page 166\n",
      "page 167\n",
      "page 168\n",
      "page 169\n",
      "page 170\n",
      "page 171\n",
      "page 172\n",
      "page 173\n",
      "page 174\n",
      "page 175\n",
      "page 176\n",
      "page 177\n",
      "page 178\n",
      "page 179\n",
      "page 180\n",
      "page 181\n",
      "page 182\n",
      "page 183\n",
      "page 184\n",
      "page 185\n",
      "page 186\n",
      "page 187\n",
      "page 188\n",
      "page 189\n",
      "page 190\n",
      "page 191\n",
      "page 192\n",
      "page 193\n",
      "page 194\n",
      "page 195\n",
      "page 196\n",
      "page 197\n",
      "page 198\n",
      "page 199\n",
      "Mean Squared Error List: [251.97280641450925, 98.08951669670881, 713.7862479793296, 370.54937039468, 126.32692852592885, 943.7003898600391, 1106.7104797188151, 1040.2946101575612]\n",
      "Coefficient list: [array([ 0.93059777, -0.15403331,  0.4505907 , -0.55343452,  0.21349275,\n",
      "       -0.47657569,  0.42959252, -0.17945126]), array([ 1.01408421, -0.19735374,  0.17427778, -0.11084391,  0.07680279,\n",
      "       -0.13278738,  0.05761198,  0.02635602, -0.08662697]), array([ 1.04333654, -0.15649835,  0.02271608, -0.01483019,  0.08014959,\n",
      "       -0.12784003,  0.06933598, -0.03874249, -0.08382027,  0.08177376]), array([ 1.08316287, -0.19542067,  0.12076063, -0.1763694 ,  0.24047442,\n",
      "       -0.14885713, -0.08754279,  0.085734  , -0.08384313,  0.07238081,\n",
      "        0.01797222]), array([ 1.07556208, -0.16481616,  0.12936775, -0.16771574,  0.13936069,\n",
      "       -0.06454985,  0.0208699 ,  0.02322899, -0.05961464,  0.06288318,\n",
      "        0.01512239, -0.01151582]), array([ 1.10762701, -0.1998144 ,  0.1423039 , -0.16335843,  0.14144553,\n",
      "       -0.07532533,  0.03880045,  0.01010272, -0.09242474,  0.0584372 ,\n",
      "        0.02837544,  0.01844252]), array([ 1.00038656, -0.03108346, -0.0350734 , -0.0202514 ,  0.11267382,\n",
      "        0.02662575, -0.16015034,  0.05288289, -0.07754212,  0.12223127,\n",
      "        0.0982839 , -0.05323497, -0.01936333]), array([ 1.01196206e+00, -8.85710177e-02,  1.05807792e-01,  3.27237531e-04,\n",
      "       -5.51881961e-02,  7.95563176e-02, -6.69671889e-02,  1.12925098e-02,\n",
      "       -1.30323211e-01,  1.56086281e-01,  4.89814553e-02, -1.08790307e-01,\n",
      "        3.45859915e-02])]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error List: [441.61748447286914, 97.1081375350955, 724.5947716500026, 528.8524808442817, 109.72218585014788, 1042.4419768155267, 1125.2833785803612, 1020.9298941867035]\n",
      "Coefficient list: [array([  0.75946592,  -0.06829101,   0.51431831,  -0.7047189 ,\n",
      "         0.3966553 ,  -0.79901978,   0.72146806,  -0.42632712,\n",
      "        15.09922693,  11.46013614, -20.80234269]), array([  1.03353685,  -0.23781244,   0.17865897,  -0.07803576,\n",
      "         0.04647133,  -0.07037317,  -0.01527394,   0.07127631,\n",
      "        -0.10189516, -10.34203523,  -0.87462713,   4.92267232]), array([ 1.04104069e+00, -1.75031672e-01,  2.83423258e-02, -1.97430116e-03,\n",
      "        6.57975127e-02, -9.53662314e-02,  4.90323458e-02, -3.56504105e-02,\n",
      "       -5.12808543e-02,  4.62334768e-02, -4.47944522e+00, -2.18227808e+00,\n",
      "        4.61307938e+00]), array([ 1.11739005e+00, -2.10771882e-01,  9.72695608e-02, -1.57180804e-01,\n",
      "        2.29690801e-01, -1.51648425e-01, -1.31772957e-01,  1.33319617e-01,\n",
      "       -1.11446419e-01,  1.18401721e-02,  6.27542491e-02, -2.65719552e+01,\n",
      "        9.26129957e+00, -1.62021190e+01]), array([ 1.10031837e+00, -1.81352339e-01,  1.19997383e-01, -1.63228296e-01,\n",
      "        1.41580553e-01, -6.73382734e-02,  9.78856153e-03,  2.98397424e-02,\n",
      "       -5.71390923e-02,  9.89942665e-03,  4.76070156e-02,  3.63666465e-03,\n",
      "       -2.27664047e+01,  5.64851064e+00, -8.21025915e+00]), array([ 1.13313556e+00, -2.15230661e-01,  1.26463946e-01, -1.59029891e-01,\n",
      "        1.46723622e-01, -7.92891223e-02,  2.28148279e-02,  1.91568682e-02,\n",
      "       -8.21179006e-02, -7.30842556e-04,  5.92104784e-02,  3.59920668e-02,\n",
      "       -2.70328280e+01,  6.28026517e+00, -6.25908007e+00]), array([ 0.99655387, -0.02783158, -0.03472464, -0.02314933,  0.10991503,\n",
      "        0.03348881, -0.16154924,  0.05898599, -0.07671482,  0.12652066,\n",
      "        0.08767537, -0.04782173, -0.02592367, -1.14578338, -3.08983418,\n",
      "       -1.32713003]), array([ 1.00644968e+00, -8.77724814e-02,  1.09086261e-01,  2.10523959e-03,\n",
      "       -6.25430836e-02,  8.42236729e-02, -5.99375259e-02,  1.01675257e-02,\n",
      "       -1.32118465e-01,  1.65539535e-01,  4.00274813e-02, -1.09142845e-01,\n",
      "        3.21552175e-02,  5.78489891e+00, -2.88447643e+00, -2.12712882e+00])]\n"
     ]
    }
   ],
   "source": [
    "def main_read_in_csv(ticker):\n",
    "    df_master = pd.read_csv('./data'+ticker+'.csv')\n",
    "    \n",
    "    ###### Per Fold do cross validation ######\n",
    "    MSE_list_AR, coef_list_AR = cross_validate_AR(df_master)   \n",
    "    print('Mean Squared Error List:', MSE_list_AR)\n",
    "    print('Coefficient list:',coef_list_AR)\n",
    "    MSE_list_ADL, coef_list_ADL = cross_validate_ADL(df_master)\n",
    "    print('Mean Squared Error List:', MSE_list_ADL)\n",
    "    print('Coefficient list:',coef_list_ADL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cross_validate_AR(df):\n",
    "    \"\"\" Runs the backtest with k fold cross validation for AR model\n",
    "    Params:\n",
    "        df (pd.Dataframe): Dataframe with X_t, lags, and sentiment score\n",
    "    Returns:\n",
    "        MSE_list (list): List of MSE per fold of cross-validation\n",
    "        coef_list (list): List of coef per fold of cross-validation\n",
    "    \"\"\"\n",
    "    tscv = TimeSeriesSplit(n_splits=8)\n",
    "    X = df.loc[:, df.columns !='X_t']\n",
    "    Y = df['X_t']\n",
    "    \n",
    "    MSE_list = []\n",
    "    coef_list = []\n",
    "        \n",
    "    for train_index, test_index in tscv.split(X):\n",
    "        X_train, X_test = X.iloc[train_index,:], X.iloc[test_index,:]\n",
    "        Y_train, Y_test = Y.iloc[train_index], Y[test_index]\n",
    "        \n",
    "        # find optimal lag period\n",
    "        lag = get_lag_period(Y_train)\n",
    "\n",
    "        # filter the lags and sentiment values\n",
    "        X_train = X_train.iloc[:,range(lag-1)]\n",
    "        X_test = X_test.iloc[:,range(lag-1)]\n",
    "\n",
    "        MSE, coef = fit_and_run_regression(X_train, Y_train, X_test, Y_test)\n",
    "        MSE_list.append(MSE)\n",
    "        coef_list.append(coef)\n",
    "        \n",
    "    return MSE_list, coef_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cross_validate_ADL(df):\n",
    "    \"\"\" Runs the backtest with k fold cross validation for ADL model\n",
    "    Params:\n",
    "        df (pd.Dataframe): Dataframe with X_t, lags, and sentiment score\n",
    "    Returns:\n",
    "        MSE_list (list): List of MSE per fold of cross-validation\n",
    "        coef_list (list): List of coef per fold of cross-validation\n",
    "    \"\"\"\n",
    "    tscv = TimeSeriesSplit(n_splits=8)\n",
    "    X = df.loc[:, df.columns !='X_t']\n",
    "    Y = df['X_t']\n",
    "    \n",
    "    MSE_list = []\n",
    "    coef_list = []\n",
    "        \n",
    "    for train_index, test_index in tscv.split(X):\n",
    "        X_train, X_test = X.iloc[train_index,:], X.iloc[test_index,:]\n",
    "        Y_train, Y_test = Y.iloc[train_index], Y[test_index]\n",
    "        \n",
    "        # find optimal lag period\n",
    "        lag = get_lag_period(Y_train)\n",
    "\n",
    "        # filter the lags and sentiment values\n",
    "        X_train = X_train.iloc[:,range(lag-1)]\n",
    "        X_test = X_test.iloc[:,range(lag-1)]\n",
    "        \n",
    "        # Add in the sentiment values\n",
    "        X_train['Pos_t-1'] = df.iloc[train_index,-3]\n",
    "        X_train['Neu_t-1'] = df.iloc[train_index,-2]\n",
    "        X_train['Neg_t-1'] = df.iloc[train_index,-1]\n",
    "        \n",
    "        X_test['Pos_t-1'] = df.iloc[test_index,-3]\n",
    "        X_test['Neu_t-1'] = df.iloc[test_index,-2]\n",
    "        X_test['Neg_t-1'] = df.iloc[test_index,-1]\n",
    "        \n",
    "        X_train = X_train.fillna(0)\n",
    "        X_test = X_test.fillna(0)\n",
    "\n",
    "        MSE, coef = fit_and_run_regression(X_train, Y_train, X_test, Y_test)\n",
    "        MSE_list.append(MSE)\n",
    "        coef_list.append(coef)\n",
    "        \n",
    "    return MSE_list, coef_list\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Methods Used for Data Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calculate_sentiment(sentence_arr):\n",
    "    \"\"\" Returns the average sentiment of the array\n",
    "    Params:\n",
    "        sentence_arr(Array): Array of setences that we have to calculate\n",
    "        the sentiment of.\n",
    "    Returns:\n",
    "        sentiment (dictionary): Takes the average of all sentences\n",
    "        format of score is {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}\n",
    "    \"\"\"\n",
    "    sia = SentimentIntensityAnalyzer()\n",
    "    df = df = pd.DataFrame(columns=['neg','neu','pos','compound'])\n",
    "    for sentence in sentence_arr:\n",
    "        sentiment = sia.polarity_scores(sentence)\n",
    "        df_sentiment = pd.DataFrame([sentiment], columns=sentiment.keys())\n",
    "        df = df.append(df_sentiment)\n",
    "    \n",
    "    avg_sentiment = dict(df.mean())    \n",
    "    return avg_sentiment "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def query_news_articles(company, start_date, end_date, trading_dates, sources):\n",
    "    \"\"\" Queries news article for a certain time frame and split it by dates\n",
    "        Note that\n",
    "    Params:\n",
    "        company (String): Name of company\n",
    "        start_date (String): Start date in format of \"2001-12-31\"\n",
    "        end_date (String): End date in format of \"2001-12-31\"\n",
    "         trading_dates (Array of Strings): Array of dates when the market was open\n",
    "                 dates in format of \"2001-12-31\"\n",
    "        sources (Array of Strings): Array of different news sources\n",
    "    Returns:\n",
    "        company_dic (dictionary): keys are date, values are array of headlines\n",
    "    \"\"\"\n",
    "    company_dict = {k: [] for k in trading_dates.date}\n",
    "    start_date = int(start_date.replace(\"-\",\"\").split('T')[0])\n",
    "    ending_date = (dateutil.parser.parse(end_date)-datetime.timedelta(days=1)).isoformat()\n",
    "    end_date = int(ending_date.replace(\"-\",\"\").split('T')[0])\n",
    "    newsdata = api.search(q=company, begin_date = start_date,\n",
    "                           end_date = end_date,\n",
    "                          fq='headline:('+company+ ') OR body:('+company+') AND source:(' + sources + ')',\n",
    "                          page = 0,\n",
    "                          facet_filter = True)\n",
    "                               \n",
    "\n",
    "    #print(newsdata) # newsdata is full HTTP response\n",
    "    number_of_hits = newsdata['response']['meta']['hits']\n",
    "    number_of_pages = (number_of_hits // 10) + 1\n",
    "    \n",
    "    time.sleep(1)\n",
    "    # page through results and add headlines to companY_dict\n",
    "    for i in range(0, min(number_of_pages,50)):\n",
    "        print('page', i)\n",
    "        newsdata = api.search(q=company, begin_date = start_date,\n",
    "                           end_date = end_date,\n",
    "                          fq='headline:('+company+ ') OR body:('+company+') AND source:(' + sources + ')',\n",
    "                          page = i,\n",
    "                          facet_filter = True)\n",
    "        articles = newsdata['response']['docs']\n",
    "        for article in articles:\n",
    "            relevance = article['score']\n",
    "            if relevance >= 0.005: \n",
    "                headline = article['headline']['main']\n",
    "                blurb = article['snippet']\n",
    "                # print(article['pub_date'], '\\t', article['headline']['main'])\n",
    "            \n",
    "                # description = article['description']\n",
    "                # format of date is 2018-04-13T00:46:59Z (UTC format)\n",
    "                publish_date = article['pub_date'] \n",
    "                # adjust date for trading day\n",
    "                publish_date, publish_time = publish_date.split('T')\n",
    "                date_arr = publish_date.split('-')\n",
    "                publish_datetime = datetime.date(int(date_arr[0]), int(date_arr[1]), int(date_arr[2]))\n",
    "                time_arr = publish_time[:-1].split(':')\n",
    "                # stock market closes at 4:00 PM EST; if article published after \n",
    "                # 16:00:00+4:00:00 = 20:00:00 UTC headline affects next trading day;\n",
    "                # otherwise affects current trading day\n",
    "                trading_datetime = publish_datetime\n",
    "                if int(time_arr[0]) >= 20:\n",
    "                    trading_datetime += datetime.timedelta(days=1)\n",
    "                \n",
    "                # if given trading_date invalid (ie if article published on Friday \n",
    "                # after market close, Saturday, or Sunday before 4 pm est) push trading_date\n",
    "                # to the following Monday (ie first valid trading_date)\n",
    "                while trading_datetime not in trading_dates:\n",
    "                    trading_datetime += datetime.timedelta(1)\n",
    "                company_dict[trading_datetime].append(headline)\n",
    "                # company_dict[trading_datetime].append(blurb) include 'snippet' in sentiment analysis\n",
    "        time.sleep(1)\n",
    "        \n",
    "    return company_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_lag_period(X_train):\n",
    "    \"\"\" Finds the optimal lab period given a ticker and the start date and end date\n",
    "    Params:\n",
    "        X_train (df): Training data used to calculate lag back\n",
    "    Returns:\n",
    "        lag (int): Number of lag periods\n",
    "    \"\"\"\n",
    "    model = AR(X_train)\n",
    "    model_fit = model.fit()\n",
    "    lag = model_fit.k_ar\n",
    "    \n",
    "    return lag\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_in_stock_data(ticker, start_date, end_date):\n",
    "    \"\"\" Initializes a data frame for a certain ticker\n",
    "    Params:\n",
    "        ticker (String): Stock ticker to be analyzed\n",
    "        start_date (String): Start date in format of \"2001-12-31\"\n",
    "        end_date (String): End date in format of \"2001-12-31\"\n",
    "        lag (int): Number of lag periods\n",
    "    Returns:\n",
    "        dataframe (pd.Dataframe): Dataframe with index 'Date' and column 'X_t'\n",
    "    \"\"\"\n",
    "    # Get the data in a dataframe\n",
    "    dataframe = web.DataReader(ticker, 'morningstar', start_date, end_date)['Close']\n",
    "    dataframe = pd.Series.to_frame(dataframe)\n",
    "    dataframe.reset_index(level=0, drop=True, inplace=True)\n",
    "    dataframe.columns = ['X_t']\n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fit_and_run_regression(X_train, Y_train, X_test, Y_test, doprint=False):\n",
    "    \"\"\" Fits the model with train and test data\n",
    "    Params:\n",
    "        X_train (pd.Dataframe): X training data\n",
    "        Y_train (pd.Dataframe): Y training data\n",
    "    Returns:\n",
    "        MSE (float): Mean Squared Error \n",
    "    \"\"\"\n",
    "    LinReg = LinearRegression(normalize=True)\n",
    "    LinReg.fit(X_train,Y_train)\n",
    "    Y_pred = LinReg.predict(X_test)\n",
    "    MSE = mean_squared_error(Y_test, Y_pred)\n",
    "    coefficients = LinReg.coef_\n",
    "    if doprint:\n",
    "        print(\"R^2 Value: %.2f\" %LinReg.score(X_test, Y_test))\n",
    "        print(\"Mean squared error: %.2f \"% MSE)\n",
    "        print(\"Coefficients for the Regression are: \" ,coefficients)\n",
    "    \n",
    "    return MSE, coefficients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try AR Regression without the package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "get_lag_period() takes 1 positional argument but 3 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-a363c66c5d19>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Try normal linear regression using lag period lags\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mlag\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_lag_period\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'AMZN'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_date\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_date\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Number of lags:'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlag\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mdf_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minitialize_dataframe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'AMZN'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_date\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_date\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlag\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mdf_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: get_lag_period() takes 1 positional argument but 3 were given"
     ]
    }
   ],
   "source": [
    "# Try normal linear regression using lag period lags\n",
    "lag = get_lag_period('AMZN', start_date, end_date)\n",
    "print('Number of lags:',lag)\n",
    "df_test = initialize_dataframe('AMZN', start_date, end_date, lag)\n",
    "df_test.head()\n",
    "\n",
    "Y = df_test['X_t']\n",
    "X = df_test.loc[:, df_test.columns !='X_t']\n",
    "X_train, X_test = X[:int(len(X)*(0.8))], X[int(len(X)*(0.8)):]\n",
    "Y_train, Y_test = Y[:int(len(Y)*(0.8))], Y[int(len(Y)*(0.8)):]\n",
    "\n",
    "LinReg = LinearRegression(normalize=True)\n",
    "LinReg.fit(X_train,Y_train)\n",
    "Y_pred = LinReg.predict(X_test)\n",
    "print(\"R^2 Value: %.2f\" %LinReg.score(X_test, Y_test))\n",
    "print(\"Mean squared error: %.2f\"\n",
    "      % mean_squared_error(Y_test, Y_pred))\n",
    "print(\"Coefficients for the Regression are: \" ,LinReg.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Y_test' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-797288d6c639>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mY_plot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mY_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdeep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mY_plot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mY_plot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_frame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mY_plot\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Predicted'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSeries\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mY_plot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mY_plot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'Actual'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Predicted'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY_plot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Y_test' is not defined"
     ]
    }
   ],
   "source": [
    "Y_plot = Y_test.copy(deep=True)\n",
    "Y_plot = Y_plot.to_frame()\n",
    "Y_plot['Predicted'] = pd.Series(Y_pred, index=Y_plot.index)\n",
    "Y_plot.columns = ['Actual', 'Predicted']\n",
    "print(Y_plot.head())\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.title('Predicted Vs. Actual Plot AR model')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Price')\n",
    "plt.plot(Y_plot.index, Y_plot['Actual'], label='Actual')\n",
    "plt.plot(Y_plot.index, Y_plot['Predicted'],label='Predicted')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try Regression with sentiment analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_sentiment = main()\n",
    "df_sentiment.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Do the regression sentiment\n",
    "\n",
    "# Count number of nulls and replace them with 0's\n",
    "df_sentiment.isnull().sum()\n",
    "df_sentiment = df_sentiment.fillna(0)\n",
    "df_sentiment.head()\n",
    "\n",
    "Y = df_sentiment['X_t']\n",
    "X = df_sentiment.loc[:, df_sentiment.columns !='X_t']\n",
    "X_train, X_test = X[:int(len(X)*(0.8))], X[int(len(X)*(0.8)):]\n",
    "Y_train, Y_test = Y[:int(len(Y)*(0.8))], Y[int(len(Y)*(0.8)):]\n",
    "\n",
    "LinReg_sentiment = LinearRegression(normalize=True)\n",
    "LinReg_sentiment.fit(X_train,Y_train)\n",
    "Y_pred = LinReg_sentiment.predict(X_test)\n",
    "print(\"R^2 Value: %.2f\" %LinReg_sentiment.score(X_test, Y_test))\n",
    "print(\"Mean squared error: %.2f\"\n",
    "      % mean_squared_error(Y_test, Y_pred))\n",
    "print(\"Coefficients for the Regression are: \" , LinReg_sentiment.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Plot\n",
    "Y_plot = Y_test.copy(deep=True)\n",
    "Y_plot = Y_plot.to_frame()\n",
    "Y_plot['Predicted'] = pd.Series(Y_pred, index=Y_plot.index)\n",
    "Y_plot.columns = ['Actual', 'Predicted']\n",
    "print(Y_plot.head())\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.title('Predicted Vs. Actual Plot ADL Model')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Price')\n",
    "plt.plot(Y_plot.index, Y_plot['Actual'], label='Actual')\n",
    "plt.plot(Y_plot.index, Y_plot['Predicted'],label='Predicted')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Run to get amazon's df\n",
    "df_current = main()\n",
    "df_current.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Find index of earlier data\n",
    "df_current.loc[datetime.datetime(2018,2,7)]\n",
    "# df_current.loc[df_current['Date'] == datetime.datetime(2018,2,7)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Use this to splice the dataframe to 2018-02-07 and after\n",
    "df_changed = df_current[456:].fillna(0)\n",
    "df_changed.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Plot autocorrelation values\n",
    "plt.figure()\n",
    "autocorrelation_plot(df_changed['X_t'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Split into train and test\n",
    "X = df_changed['X_t']\n",
    "train, test = X[:int(len(X)*(0.8))], X[int(len(X)*(0.8)):]\n",
    "# Fit the model\n",
    "model = AR(train)\n",
    "model_fit = model.fit()\n",
    "print('Lag: %s' % model_fit.k_ar)\n",
    "print('Coefficients: %s' % model_fit.params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# make predictions\n",
    "predictions = model_fit.predict(start=len(train), end=len(train)+len(test)-1, dynamic=False)\n",
    "for i in range(len(predictions)):\n",
    "    print('predicted=%f, expected=%f' % (predictions[i], test[i]))\n",
    "error = mean_squared_error(test, predictions)\n",
    "print('Test MSE: %.3f' % error)\n",
    "R2 = r2_score(test, predictions)\n",
    "print('Test R^2 Score: %.3f ' % R2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Plot the preditions\n",
    "plt.figure(figsize=(10,7))\n",
    "plt.plot(test, label='Actual Values')\n",
    "plt.title('Predicted vs. Actual Values')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Price')\n",
    "plt.plot(predictions, label='Predicted Values')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [anaconda]",
   "language": "python",
   "name": "Python [anaconda]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
